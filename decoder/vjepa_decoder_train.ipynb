{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# V-JEPA 2 Visualizer Decoder\n",
                "Trains a CNN decoder that maps V-JEPA 2 latent patch tokens → reconstructed video frames.\n",
                "\n",
                "**Runtime:** Set to **T4 GPU** via Runtime → Change runtime type"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Install dependencies\n",
                "!pip install -q transformers accelerate yt-dlp opencv-python-headless"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Load V-JEPA 2 encoder (frozen)\n",
                "import torch\n",
                "from transformers import AutoModel\n",
                "\n",
                "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f'Device: {DEVICE}')\n",
                "\n",
                "encoder = AutoModel.from_pretrained('facebook/vjepa2-vitl-fpc64-256', trust_remote_code=True)\n",
                "encoder = encoder.to(DEVICE, dtype=torch.float16).eval()\n",
                "for p in encoder.parameters():\n",
                "    p.requires_grad = False\n",
                "\n",
                "print('Encoder loaded and frozen')\n",
                "print(f'Params: {sum(p.numel() for p in encoder.parameters()):,}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Build training dataset\n",
                "# Download Big Buck Bunny, extract 8-frame clips, encode with V-JEPA 2\n",
                "import os, subprocess, cv2, numpy as np\n",
                "from torchvision import transforms\n",
                "from pathlib import Path\n",
                "from PIL import Image\n",
                "\n",
                "DDIR = Path('/tmp/jd')\n",
                "DDIR.mkdir(exist_ok=True)\n",
                "\n",
                "ET = transforms.Compose([\n",
                "    transforms.Resize((256, 256)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "TT = transforms.Compose([\n",
                "    transforms.Resize((256, 256)),\n",
                "    transforms.ToTensor()\n",
                "])\n",
                "\n",
                "# Download video\n",
                "vp = '/tmp/bbb.mp4'\n",
                "if not os.path.exists(vp) or os.path.getsize(vp) < 10000:\n",
                "    print('Downloading Big Buck Bunny...')\n",
                "    subprocess.run([\n",
                "        'yt-dlp', '--quiet',\n",
                "        '-f', 'bestvideo[height<=360][ext=mp4]/best[height<=360]',\n",
                "        '--download-sections', '*0:00-0:40',\n",
                "        '-o', vp,\n",
                "        'https://www.youtube.com/watch?v=_FjuOVeahA8'\n",
                "    ], timeout=90)\n",
                "\n",
                "# Extract clips and encode\n",
                "cap = cv2.VideoCapture(vp)\n",
                "total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
                "starts = np.linspace(0, max(0, total - 60), 40, dtype=int)\n",
                "n_saved = 0\n",
                "\n",
                "for ci, s in enumerate(starts):\n",
                "    fe, ft = [], []\n",
                "    for t in range(8):\n",
                "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(s + t * 4))\n",
                "        ret, fr = cap.read()\n",
                "        if not ret:\n",
                "            break\n",
                "        p = Image.fromarray(cv2.cvtColor(fr, cv2.COLOR_BGR2RGB))\n",
                "        fe.append(ET(p))\n",
                "        ft.append(TT(p))\n",
                "    if len(fe) < 8:\n",
                "        continue\n",
                "    vid = torch.stack(fe).permute(1, 0, 2, 3).unsqueeze(0).to(DEVICE, dtype=torch.float16)\n",
                "    with torch.no_grad():\n",
                "        emb = encoder(pixel_values_videos=vid).last_hidden_state[0].cpu().float()\n",
                "    torch.save({'e': emb, 't': ft[3]}, DDIR / f'p{ci:04d}.pt')\n",
                "    n_saved += 1\n",
                "\n",
                "cap.release()\n",
                "print(f'Dataset: {n_saved} pairs saved to {DDIR}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Define CNN Decoder + train 25 epochs\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "class DS(Dataset):\n",
                "    def __init__(self, d):\n",
                "        self.f = sorted(Path(d).glob('p*.pt'))\n",
                "    def __len__(self):\n",
                "        return len(self.f)\n",
                "    def __getitem__(self, i):\n",
                "        d = torch.load(self.f[i])\n",
                "        return d['e'], d['t']\n",
                "\n",
                "loader = DataLoader(DS('/tmp/jd'), batch_size=4, shuffle=True)\n",
                "print(f'Loader: {len(loader)} batches/epoch')\n",
                "\n",
                "\n",
                "class Decoder(nn.Module):\n",
                "    \"\"\"\n",
                "    Maps V-JEPA 2 tokens [B, 1024, 1024] -> RGB frame [B, 3, 256, 256]\n",
                "    Token layout: 1024 = 4 temporal x 16 spatial x 16 spatial\n",
                "    \"\"\"\n",
                "    def __init__(self, D=1024, H=384):\n",
                "        super().__init__()\n",
                "        self.proj = nn.Sequential(\n",
                "            nn.Linear(D, H),\n",
                "            nn.LayerNorm(H),\n",
                "            nn.GELU()\n",
                "        )\n",
                "\n",
                "        def up(i, o):\n",
                "            return nn.Sequential(\n",
                "                nn.ConvTranspose2d(i, o, 4, 2, 1),\n",
                "                nn.GroupNorm(8, o),\n",
                "                nn.GELU(),\n",
                "                nn.Conv2d(o, o, 3, padding=1),\n",
                "                nn.GroupNorm(8, o),\n",
                "                nn.GELU()\n",
                "            )\n",
                "\n",
                "        self.cnn = nn.Sequential(\n",
                "            up(H, 256),    # 16 -> 32\n",
                "            up(256, 128),  # 32 -> 64\n",
                "            up(128, 64),   # 64 -> 128\n",
                "            up(64, 32),    # 128 -> 256\n",
                "            nn.Conv2d(32, 3, 3, padding=1),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        B = x.shape[0]\n",
                "        # Project + temporal mean pool: [B,1024,D] -> [B,16,16,H] -> [B,H,16,16]\n",
                "        x = self.proj(x).view(B, 4, 16, 16, -1).mean(1).permute(0, 3, 1, 2)\n",
                "        return self.cnn(x)\n",
                "\n",
                "\n",
                "dec = Decoder().to(DEVICE)\n",
                "print(f'Decoder params: {sum(p.numel() for p in dec.parameters()):,}')\n",
                "\n",
                "opt = torch.optim.AdamW(dec.parameters(), lr=2e-4, weight_decay=1e-4)\n",
                "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=25)\n",
                "mse = nn.MSELoss()\n",
                "\n",
                "def loss_fn(p, t):\n",
                "    pixel_loss = mse(p, t)\n",
                "    # Gradient sharpness term\n",
                "    dy_p = p[:, :, 1:] - p[:, :, :-1]\n",
                "    dy_t = t[:, :, 1:] - t[:, :, :-1]\n",
                "    dx_p = p[:, :, :, 1:] - p[:, :, :, :-1]\n",
                "    dx_t = t[:, :, :, 1:] - t[:, :, :, :-1]\n",
                "    return pixel_loss + 0.1 * (mse(dy_p, dy_t) + mse(dx_p, dx_t))\n",
                "\n",
                "\n",
                "# --- Train ---\n",
                "print('Training...')\n",
                "hist = []\n",
                "for ep in range(25):\n",
                "    dec.train()\n",
                "    el = 0\n",
                "    for e, t in loader:\n",
                "        e, t = e.to(DEVICE), t.to(DEVICE)\n",
                "        p = dec(e)\n",
                "        l = loss_fn(p, t)\n",
                "        opt.zero_grad()\n",
                "        l.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(dec.parameters(), 1.0)\n",
                "        opt.step()\n",
                "        el += l.item()\n",
                "    sched.step()\n",
                "    hist.append(el / len(loader))\n",
                "    if (ep + 1) % 5 == 0:\n",
                "        print(f'Epoch {ep+1:2d}/25 | loss={hist[-1]:.5f}')\n",
                "\n",
                "torch.save(dec.state_dict(), '/tmp/vjepa_dec.pt')\n",
                "print('Weights saved to /tmp/vjepa_dec.pt')\n",
                "\n",
                "# Loss curve\n",
                "plt.figure(figsize=(7, 2.5))\n",
                "plt.plot(hist, lw=2, color='steelblue')\n",
                "plt.grid(alpha=0.3)\n",
                "plt.title('V-JEPA 2 Decoder — Training Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Visualise reconstructions\n",
                "dec.eval()\n",
                "ds = DS('/tmp/jd')\n",
                "idxs = np.random.choice(len(ds), 5, replace=False)\n",
                "\n",
                "fig, ax = plt.subplots(2, 5, figsize=(18, 7))\n",
                "for col, idx in enumerate(idxs):\n",
                "    e, t = ds[idx]\n",
                "    with torch.no_grad():\n",
                "        pred = dec(e.unsqueeze(0).to(DEVICE))[0].cpu()\n",
                "    orig = t.permute(1, 2, 0).numpy()\n",
                "    rec = pred.clamp(0, 1).permute(1, 2, 0).numpy()\n",
                "    psnr = -10 * np.log10(((orig - rec) ** 2).mean() + 1e-8)\n",
                "\n",
                "    ax[0, col].imshow(orig)\n",
                "    ax[0, col].set_title(f'Original #{idx}', fontsize=8)\n",
                "    ax[0, col].axis('off')\n",
                "\n",
                "    ax[1, col].imshow(rec)\n",
                "    ax[1, col].set_title(f'PSNR {psnr:.1f} dB', fontsize=8)\n",
                "    ax[1, col].axis('off')\n",
                "\n",
                "ax[0, 0].set_ylabel('Ground Truth', fontsize=10)\n",
                "ax[1, 0].set_ylabel('V-JEPA 2 → Decoder', fontsize=10)\n",
                "plt.suptitle(\n",
                "    'V-JEPA 2 Latent Space → Reconstructed Frames\\n'\n",
                "    '(CNN Decoder trained on MSE + Gradient Loss, 25 epochs)',\n",
                "    fontsize=12, fontweight='bold'\n",
                ")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "print('Done!')"
            ]
        }
    ]
}