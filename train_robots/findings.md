# V-JEPA 2 ‚Üí Robot Policy ‚Äî Findings

**Goal:** Assess whether V-JEPA 2, a video foundation model pretrained on internet video, can serve as a frozen perception backbone for robot control ‚Äî without any robot-specific encoder training.

**Task:** dm_control `reacher-easy` ‚Äî a 2-joint robot arm must move its fingertip to a random target.

---

## Experiment Pipeline

```
Camera frames (last 8) ‚Üí [V-JEPA 2, frozen, 325M params] ‚Üí 1024-dim embedding
                                                                    ‚Üì
                                                       [MLP policy, 279k params]
                                                                    ‚Üì
                                                         joint torques [2]
```

### Phase 1 steps
1. **Expert demo generation** ‚Äî scripted P-controller runs 500 episodes in dm_control, rendering 256√ó256 pixel frames
2. **V-JEPA encoding** ‚Äî each 8-frame sliding window encoded to a 1024-dim embedding (Modal A10G, batch_size=32)
3. **Behavior cloning** ‚Äî MLP trained on (embedding, expert_action) pairs via MSE loss
4. **Evaluation** ‚Äî trained policy rolled out for 100 live episodes with V-JEPA encoding at every step

**Dataset:** 100,000 transitions (500 episodes √ó 200 steps), 410MB of embeddings stored in Modal volume.

---

## Phase 1 Results ‚Äî Behavior Cloning

**Date:** 2026-02-24 | **Eval episodes:** 100 per condition

| Condition | Success Rate | Mean Reward | Notes |
|---|---|---|---|
| **V-JEPA BC** (ours) | **20.0%** | **66.4** | Frozen V-JEPA 2 + 3-layer MLP |
| Scripted Expert | 20.0% | 67.6 | Upper bound (P-controller) |
| Random Policy | ~2% | ~0 | Lower bound |

### BC Training
- Architecture: `Linear(1024‚Üí256) ‚Üí LayerNorm ‚Üí ReLU ‚Üí Linear(256‚Üí64) ‚Üí ReLU ‚Üí Linear(64‚Üí2) ‚Üí Tanh`
- Parameters: 279,490 (tiny)
- Train MSE loss: 0.0001 ‚Üí Val MSE: 0.0002 (100 epochs, cosine LR decay)
- Training time: ~8 min on CPU

### Key findings

**‚úÖ V-JEPA 2 embeddings fully support policy learning.** The BC policy matches the scripted expert exactly (20% vs 20%), while the random policy baseline achieves only ~2%. That's a **10√ó lift over the random baseline** with a frozen encoder that has seen zero robot data.

**‚ö†Ô∏è The ceiling is the expert, not the encoder.** The P-controller expert itself only achieves ~20% because it applies Cartesian direction forces to joint-space actuators ‚Äî a fundamental mismatch. BC cannot exceed what it learns from. The V-JEPA embedding quality is not the bottleneck.

**üí° Implication:** V-JEPA 2 encodes enough spatial and directional information for a tiny MLP to replicate a controller's behavior. With a better expert (or RL), the success rate should improve significantly.

---

## What V-JEPA 2 Encodes (from linear probe, see findings/findings.md)

| Property | V-JEPA R¬≤ / Acc | Random Baseline |
|---|---|---|
| Object XY position | **R¬≤=0.86** | -0.11 |
| Object size/depth | **R¬≤=0.89** | -0.11 |
| Object class | **88.4%** | 42.3% |
| Motion direction | 63.6% | 49.9% |

The reacher task primarily requires XY spatial awareness ‚Äî exactly where V-JEPA is strongest.

---

## Scaling Trend

| Scale | Expert SR | BC SR | Notes |
|---|---|---|---|
| Phase 1 BC | 20% | 20% | P-controller ceiling |
| Phase 2 SAC (planned) | N/A | 60-80%? | RL exceeds the expert |

---

## Phase 2 ‚Äî Exploration Data Collection

**Date:** 2026-02-25 | **Compute:** Modal A10G, ~40 min, ~$0.75

Instead of pursuing SAC directly, we pivoted to building a **world model** ‚Äî teaching the system to predict the future in latent space, then planning by imagining.

**Data pipeline:** Modified `generate_and_encode_modal.py` with epsilon-greedy noise (Œµ=0.3) injected into the P-controller. 30% of actions are random, ensuring the dataset covers diverse (including "wrong") transitions.

| Metric | Value |
|---|---|
| Episodes | 500 |
| Steps per episode | 200 |
| Total transitions | 100,000 |
| Dataset size | 496 MB |
| Format | `(z_t, a_t, z_{t+1})` triples |

---

## Phase 2b ‚Äî Large-Scale Data Collection (5000 episodes)

**Date:** 2026-02-25 | **Compute:** Modal 10 √ó A10G parallel, ~90 min wall time, ~$7

Phase 4d revealed that the 10.5M ResBlock dynamics model overfits on 80k transitions. Fix: collect 10√ó more data using parallel workers.

### Pipeline
- Parallelized across **10 Modal A10G containers** using `.map()`, each generating 500 episodes with unique seeds
- Same epsilon-greedy P-controller (Œµ=0.3)
- Shards merged on cloud (32GB RAM container) to avoid local OOM
- Uploaded directly from Modal to HuggingFace (no local download of 8GB file)

| Metric | Phase 2 | Phase 2b |
|---|---|---|
| Episodes | 500 | **5,000** |
| Transitions | 100,000 | **1,000,000** |
| Dataset size | 496 MB | **8.2 GB** |
| Avg success rate | ~15% | 12.0% |
| Wall time | 40 min (1 GPU) | 90 min (10 GPUs) |

**‚úÖ Dataset stored at:** [HuggingFace: ThomasTheMaker/vjepa2-reacher-world-model](https://huggingface.co/datasets/ThomasTheMaker/vjepa2-reacher-world-model)

---

## Phase 3 ‚Äî Action-Conditioned Dynamics Predictor

**Date:** 2026-02-25 | **Compute:** Local CPU, ~10 min

Trained an MLP to predict the next latent state: `f(z_t, a_t) ‚Üí z_{t+1}`.

### Architecture
```
Input: concat(z_t [1024], a_t [2]) = 1026-dim
 ‚Üí Linear(1026‚Üí512) ‚Üí LayerNorm ‚Üí ReLU
 ‚Üí Linear(512‚Üí512) ‚Üí LayerNorm ‚Üí ReLU
 ‚Üí Linear(512‚Üí512) ‚Üí LayerNorm ‚Üí ReLU
 ‚Üí Linear(512‚Üí1024)
Output: z_t + delta_z (residual prediction)
```

### Training
- **Loss:** SmoothL1 (Huber), AdamW, cosine LR
- **Train/Val Loss:** ~0.01 / ~0.01 (100 epochs)
- **Parameters:** ~1.2M

**‚úÖ Key finding:** Residual prediction (`z_t + Œîz`) converged faster and more stably than direct prediction. The dynamics model learned meaningful 1-step transitions.

---

## Phase 4 ‚Äî Random Shooting MPC

**Date:** 2026-02-25 | **Compute:** Local CPU, ~30 min

First MPC attempt: sample 1000 random action sequences (horizon=10), unroll each through the dynamics model, pick the one closest to `z_goal`.

| Metric | Result |
|---|---|
| Total Env Reward | **0.00** |
| Steps | 100 |

**‚ùå Failed.** Two causes identified:
1. **Bad goal state** ‚Äî random exploration couldn't find a high-reward state, so the "goal" was an arbitrary frame
2. **Random shooting is inefficient** ‚Äî sampling 1000 random trajectories in continuous 2D action space has poor coverage

---

## Phase 4b ‚Äî CEM Planner (upgraded)

**Date:** 2026-02-25 | **Compute:** Local CPU, ~60 min

Two fixes applied:
1. **Expert-generated goal** ‚Äî P-controller on a separate raw (unwrapped) env finds the closest-to-target frame (0.07 distance)
2. **Cross-Entropy Method (CEM)** ‚Äî iteratively refines action distribution over 5 iterations (500 samples, top 50 elites, warm-starting)

### Results

| Metric | Phase 4 (Random) | Phase 4b (CEM) |
|---|---|---|
| Initial Latent Dist | N/A | 16.18 |
| Final Latent Dist | N/A | 9.50 |
| **Min Latent Dist** | N/A | **7.80** |
| **Improvement** | ‚Äî | **41.3%** |
| Total Env Reward | 0.00 | **6.00** |

### Key findings

**‚úÖ The dynamics model learned real physics.** Latent distance to goal decreased 41.3% ‚Äî the planner genuinely steers the robot toward the target by imagining futures. This would not work if `f(z_t, a_t)` was noise.

**‚úÖ CEM >> random shooting.** Iterative refinement finds much better trajectories ‚Äî total env reward improved from 0.0 to 6.0.

**‚ö†Ô∏è Compounding error is the bottleneck.** The latent distance oscillates between 8-18 instead of converging monotonically. The dynamics model was trained on **1-step prediction** but the planner unrolls it for **10 steps** ‚Äî small per-step errors compound exponentially. 

**‚ö†Ô∏è dm_control `reacher-easy` reward is very sparse.** The reward only fires when the fingertip is within a very tight tolerance of the target. Even the P-controller expert gets 0 reward over 200 steps with a gain of 5.0. Latent distance is a more informative metric for this task.

**üí° Next step:** Train the dynamics model with **multi-step rollout loss** ‚Äî backpropagate through H-step unrolls during training to directly penalize compound error. Then consider Phase 5 (Dreamer-style actor-critic on imagined rollouts).

---

## Phase 4c ‚Äî Multi-Step Rollout Training

**Date:** 2026-02-25 | **Compute:** Modal A10G, ~15 min

Hypothesis: the Phase 4b oscillation was caused by compounding 1-step prediction error over 10-step planner horizon. Fix: retrain dynamics model with **multi-step rollout loss** (H=5), backpropagating through predicted states:

```
z1_pred = f(z0, a0)           ‚Üí loss vs z1
z2_pred = f(z1_pred, a1)      ‚Üí loss vs z2  (uses PREDICTED z!)
z3_pred = f(z2_pred, a2)      ‚Üí loss vs z3
```

### Results

| Metric | Phase 4b (1-step) | Phase 4c (H=5) |
|---|---|---|
| Initial Latent Dist | 16.18 | 15.42 |
| Final Latent Dist | 9.50 | 10.57 |
| **Min Latent Dist** | **7.80** | 9.55 |
| **Improvement** | **41.3%** | 31.4% |
| Total Env Reward | 6.00 | 0.00 |

### Key findings

**‚ùå Multi-step training regressed!** The 1-step model (Phase 4b) outperformed the multi-step model (Phase 4c) on every metric. The multi-step model's min latent distance (9.55) never reached Phase 4b's (7.80).

**üí° Why?** Likely causes:
1. **Training instability** ‚Äî multi-step rollout loss creates long gradient chains that are harder to optimize. The model may learn conservative predictions that don't compound errors but also lack precision.
2. **Architecture bottleneck** ‚Äî the MLP dynamics model may not have enough capacity to benefit from multi-step supervision. A Transformer or larger model may be needed.
3. **Data diversity** ‚Äî 500 episodes of epsilon-greedy P-controller may not cover enough of the state space for multi-step learning.

**üí° Conclusion:** The dynamics model architecture (simple MLP) is likely the real bottleneck, not the training objective. Phase 5 (Dreamer-style actor-critic) should use a more expressive world model, or skip model-based planning entirely and use the learned latents for model-free RL.

---

## Phase 4d ‚Äî ResBlock Dynamics Architecture

**Date:** 2026-02-25 | **Compute:** Modal A10G, ~20 min

Hypothesis: the 1.2M-param MLP lacks capacity to model dynamics accurately. Fix: 4√ó residual blocks with 1024 hidden dim ‚Üí **10.5M params** (8.5√ó larger).

### Architecture
```
Input: concat(z_t, a_t) [1026]
 ‚Üí Linear(1026‚Üí1024) ‚Üí LayerNorm ‚Üí ReLU
 ‚Üí ResBlock(1024): Linear‚ÜíLN‚ÜíReLU‚ÜíLinear‚ÜíLN + skip
 ‚Üí ResBlock(1024): ...
 ‚Üí ResBlock(1024): ...
 ‚Üí ResBlock(1024): ...
 ‚Üí Linear(1024) ‚Üí delta_z
Output: z_t + delta_z (residual prediction)
```

### Training
- **Epochs:** 150 (best val at ~epoch 40)
- **Train loss:** 0.0180 ‚Üí 0.0016 (memorized training set)
- **Val loss:** 0.0132 ‚Üí 0.0137 (worse than MLP's 0.01!)
- **Severe overfitting** ‚Äî 10.5M params on 80k samples

### Results

| Metric | 4b (MLP 1.2M) | 4c (MLP multi-step) | 4d (ResBlock 10.5M) |
|---|---|---|---|
| Min Latent Dist | **7.80** | 9.55 | 9.22 |
| **Improvement** | **41.3%** | 31.4% | 11.5% |
| Env Reward | **6.0** | 0.0 | 0.0 |

### Key findings

**‚ùå Bigger is not better.** The 10.5M ResBlock model dramatically overfits on 80k transitions and performs worst of all variants (11.5% vs 41.3%).

**‚úÖ The original small MLP is the sweet spot.** The 1.2M-param model with 1-step loss (Phase 4b) remains the best dynamics model. Its limited capacity actually acts as implicit regularization.

**üí° The real bottleneck is not the model ‚Äî it's the data.** With only 80k transitions (500 episodes), larger models memorize rather than generalize. Options:
1. **Collect more data** (5000+ episodes with more diverse policies)
2. **Add regularization** (dropout, stronger weight decay)
3. **Skip model-based planning** and use V-JEPA latents directly for model-free RL (Phase 5)

---

## Phase 4e ‚Äî Retrain on 1M Transitions (5000 episodes)

**Date:** 2026-02-25 | **Compute:** Modal A10G, ~3.5 hrs total (training + eval), ~$3.50

Hypothesis: 10√ó more data (1M vs 80k transitions) will fix ResBlock overfitting and improve both models.

### Training Results

| Metric | MLP (80k) | MLP (1M) | ResBlock (80k) | ResBlock (1M) |
|---|---|---|---|---|
| Best Val Loss | 0.0100 | **0.0039** | 0.0137 (‚Üë overfit) | **0.0057** |
| Overfitting? | Mild | ‚úÖ None | ‚ùå Severe | ‚úÖ Fixed |

### CEM Evaluation Results

| Metric | 4b (MLP, 80k) | 4e MLP (1M) | 4d (ResBlock, 80k) | 4e ResBlock (1M) |
|---|---|---|---|---|
| Init Latent Dist | 16.18 | 15.42 | 15.42 | 15.42 |
| Final Latent Dist | 9.50 | 11.48 | 13.64 | **8.63** |
| **Min Latent Dist** | **7.80** | 7.63 | 9.22 | **8.43** |
| **Improvement** | 41.3% | 25.5% | 11.5% | **44.1%** |
| **Env Reward** | 6.0 | **29.0** | 0.0 | 0.0 |

### Key findings

**‚úÖ ResBlock overfitting completely fixed.** With 1M transitions, the ResBlock (10.5M params) went from 11.5% ‚Üí **44.1% improvement** ‚Äî the best latent distance improvement across all phases. Dropout + 10√ó data eliminated the overfitting problem.

**ü§î MLP regression in latent distance but huge reward gain.** The MLP's latent distance improvement dropped (41.3% ‚Üí 25.5%), but its **environment reward jumped to 29.0** ‚Äî by far the highest of any model. This suggests the MLP with more data learned physically meaningful dynamics (real reward) even if the latent distance metric looks worse.

**üí° Latent distance ‚â† task reward.** The MLP scored 29 env reward despite "worse" latent distance, while ResBlock scored 0 reward despite better latent distance. This is a critical insight ‚Äî optimizing latent distance may not optimize for the actual task.

**üí° Implications for Phase 5:**
1. The MLP (1.2M) with 1M data is the best dynamics model for real task performance (highest env reward)
2. The ResBlock may be better at latent prediction but worse at control ‚Äî possible overfitting to latent space structure rather than task-relevant dynamics
3. Phase 5 (Dreamer actor-critic) should use the **MLP dynamics model** since it produces the best real-world outcomes

---

## Blockers / Limitations

| Issue | Status | Impact |
|---|---|---|
| P-controller expert suboptimal | Known ‚Äî by design | Caps BC at 20% |
| dm_control reacher reward very sparse | Known | Latent distance ‚â† task reward (confirmed in 4e) |
| Dynamics model compound error | Phase 4e best at 44.1% | 10√ó data helps dramatically |
| Latent dist vs env reward mismatch | **New ‚Äî identified in 4e** | MLP scores 29 reward at 25.5% improvement; ResBlock scores 0 at 44.1% |

---

## Cost Summary

| Step | Compute | Cost |
|---|---|---|
| Phase 1a: 500 demos + encoding | Modal A10G, ~35 min | ~$0.65 |
| Phase 1b: BC training | Local CPU, ~8 min | $0 |
| Phase 1c: Eval (2 conditions) | Modal A10G, ~60 min | ~$1.10 |
| Phase 2: Exploration data | Modal A10G, ~40 min | ~$0.75 |
| Phase 2b: 5000 ep parallel collection | Modal 10√óA10G, ~90 min | ~$7.00 |
| Phase 3: Dynamics training | Local CPU, ~10 min | $0 |
| Phase 4: Random shooting MPC | Local CPU, ~30 min | $0 |
| Phase 4b: CEM planner | Local CPU, ~60 min | $0 |
| Phase 4c: Multi-step dynamics | Modal A10G, ~15 min | ~$0.30 |
| Phase 4d: ResBlock dynamics | Modal A10G, ~20 min | ~$0.30 |
| Phase 4e: Retrain + eval (1M data) | Modal A10G, ~3.5 hrs | ~$3.50 |
| **Total** | | **~$13.60** |

