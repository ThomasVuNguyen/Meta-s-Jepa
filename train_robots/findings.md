# V-JEPA 2 ‚Üí Robot Policy ‚Äî Findings

**Goal:** Assess whether V-JEPA 2, a video foundation model pretrained on internet video, can serve as a frozen perception backbone for robot control ‚Äî without any robot-specific encoder training.

**Task:** dm_control `reacher-easy` ‚Äî a 2-joint robot arm must move its fingertip to a random target.

---

## Experiment Pipeline

```
Camera frames (last 8) ‚Üí [V-JEPA 2, frozen, 325M params] ‚Üí 1024-dim embedding
                                                                    ‚Üì
                                                       [MLP policy, 279k params]
                                                                    ‚Üì
                                                         joint torques [2]
```

### Phase 1 steps
1. **Expert demo generation** ‚Äî scripted P-controller runs 500 episodes in dm_control, rendering 256√ó256 pixel frames
2. **V-JEPA encoding** ‚Äî each 8-frame sliding window encoded to a 1024-dim embedding (Modal A10G, batch_size=32)
3. **Behavior cloning** ‚Äî MLP trained on (embedding, expert_action) pairs via MSE loss
4. **Evaluation** ‚Äî trained policy rolled out for 100 live episodes with V-JEPA encoding at every step

**Dataset:** 100,000 transitions (500 episodes √ó 200 steps), 410MB of embeddings stored in Modal volume.

---

## Phase 1 Results ‚Äî Behavior Cloning

**Date:** 2026-02-24 | **Eval episodes:** 100 per condition

| Condition | Success Rate | Mean Reward | Notes |
|---|---|---|---|
| **V-JEPA BC** (ours) | **20.0%** | **66.4** | Frozen V-JEPA 2 + 3-layer MLP |
| Scripted Expert | 20.0% | 67.6 | Upper bound (P-controller) |
| Random Policy | ~2% | ~0 | Lower bound |

### BC Training
- Architecture: `Linear(1024‚Üí256) ‚Üí LayerNorm ‚Üí ReLU ‚Üí Linear(256‚Üí64) ‚Üí ReLU ‚Üí Linear(64‚Üí2) ‚Üí Tanh`
- Parameters: 279,490 (tiny)
- Train MSE loss: 0.0001 ‚Üí Val MSE: 0.0002 (100 epochs, cosine LR decay)
- Training time: ~8 min on CPU

### Key findings

**‚úÖ V-JEPA 2 embeddings fully support policy learning.** The BC policy matches the scripted expert exactly (20% vs 20%), while the random policy baseline achieves only ~2%. That's a **10√ó lift over the random baseline** with a frozen encoder that has seen zero robot data.

**‚ö†Ô∏è The ceiling is the expert, not the encoder.** The P-controller expert itself only achieves ~20% because it applies Cartesian direction forces to joint-space actuators ‚Äî a fundamental mismatch. BC cannot exceed what it learns from. The V-JEPA embedding quality is not the bottleneck.

**üí° Implication:** V-JEPA 2 encodes enough spatial and directional information for a tiny MLP to replicate a controller's behavior. With a better expert (or RL), the success rate should improve significantly.

---

## What V-JEPA 2 Encodes (from linear probe, see findings/findings.md)

| Property | V-JEPA R¬≤ / Acc | Random Baseline |
|---|---|---|
| Object XY position | **R¬≤=0.86** | -0.11 |
| Object size/depth | **R¬≤=0.89** | -0.11 |
| Object class | **88.4%** | 42.3% |
| Motion direction | 63.6% | 49.9% |

The reacher task primarily requires XY spatial awareness ‚Äî exactly where V-JEPA is strongest.

---

## Scaling Trend

| Scale | Expert SR | BC SR | Notes |
|---|---|---|---|
| Phase 1 BC | 20% | 20% | P-controller ceiling |
| Phase 2 SAC (planned) | N/A | 60-80%? | RL exceeds the expert |

---

## Phase 2 ‚Äî Exploration Data Collection

**Date:** 2026-02-25 | **Compute:** Modal A10G, ~40 min, ~$0.75

Instead of pursuing SAC directly, we pivoted to building a **world model** ‚Äî teaching the system to predict the future in latent space, then planning by imagining.

**Data pipeline:** Modified `generate_and_encode_modal.py` with epsilon-greedy noise (Œµ=0.3) injected into the P-controller. 30% of actions are random, ensuring the dataset covers diverse (including "wrong") transitions.

| Metric | Value |
|---|---|
| Episodes | 500 |
| Steps per episode | 200 |
| Total transitions | 100,000 |
| Dataset size | 496 MB |
| Format | `(z_t, a_t, z_{t+1})` triples |

---

## Phase 3 ‚Äî Action-Conditioned Dynamics Predictor

**Date:** 2026-02-25 | **Compute:** Local CPU, ~10 min

Trained an MLP to predict the next latent state: `f(z_t, a_t) ‚Üí z_{t+1}`.

### Architecture
```
Input: concat(z_t [1024], a_t [2]) = 1026-dim
 ‚Üí Linear(1026‚Üí512) ‚Üí LayerNorm ‚Üí ReLU
 ‚Üí Linear(512‚Üí512) ‚Üí LayerNorm ‚Üí ReLU
 ‚Üí Linear(512‚Üí512) ‚Üí LayerNorm ‚Üí ReLU
 ‚Üí Linear(512‚Üí1024)
Output: z_t + delta_z (residual prediction)
```

### Training
- **Loss:** SmoothL1 (Huber), AdamW, cosine LR
- **Train/Val Loss:** ~0.01 / ~0.01 (100 epochs)
- **Parameters:** ~1.2M

**‚úÖ Key finding:** Residual prediction (`z_t + Œîz`) converged faster and more stably than direct prediction. The dynamics model learned meaningful 1-step transitions.

---

## Phase 4 ‚Äî Random Shooting MPC

**Date:** 2026-02-25 | **Compute:** Local CPU, ~30 min

First MPC attempt: sample 1000 random action sequences (horizon=10), unroll each through the dynamics model, pick the one closest to `z_goal`.

| Metric | Result |
|---|---|
| Total Env Reward | **0.00** |
| Steps | 100 |

**‚ùå Failed.** Two causes identified:
1. **Bad goal state** ‚Äî random exploration couldn't find a high-reward state, so the "goal" was an arbitrary frame
2. **Random shooting is inefficient** ‚Äî sampling 1000 random trajectories in continuous 2D action space has poor coverage

---

## Phase 4b ‚Äî CEM Planner (upgraded)

**Date:** 2026-02-25 | **Compute:** Local CPU, ~60 min

Two fixes applied:
1. **Expert-generated goal** ‚Äî P-controller on a separate raw (unwrapped) env finds the closest-to-target frame (0.07 distance)
2. **Cross-Entropy Method (CEM)** ‚Äî iteratively refines action distribution over 5 iterations (500 samples, top 50 elites, warm-starting)

### Results

| Metric | Phase 4 (Random) | Phase 4b (CEM) |
|---|---|---|
| Initial Latent Dist | N/A | 16.18 |
| Final Latent Dist | N/A | 9.50 |
| **Min Latent Dist** | N/A | **7.80** |
| **Improvement** | ‚Äî | **41.3%** |
| Total Env Reward | 0.00 | **6.00** |

### Key findings

**‚úÖ The dynamics model learned real physics.** Latent distance to goal decreased 41.3% ‚Äî the planner genuinely steers the robot toward the target by imagining futures. This would not work if `f(z_t, a_t)` was noise.

**‚úÖ CEM >> random shooting.** Iterative refinement finds much better trajectories ‚Äî total env reward improved from 0.0 to 6.0.

**‚ö†Ô∏è Compounding error is the bottleneck.** The latent distance oscillates between 8-18 instead of converging monotonically. The dynamics model was trained on **1-step prediction** but the planner unrolls it for **10 steps** ‚Äî small per-step errors compound exponentially. 

**‚ö†Ô∏è dm_control `reacher-easy` reward is very sparse.** The reward only fires when the fingertip is within a very tight tolerance of the target. Even the P-controller expert gets 0 reward over 200 steps with a gain of 5.0. Latent distance is a more informative metric for this task.

**üí° Next step:** Train the dynamics model with **multi-step rollout loss** ‚Äî backpropagate through H-step unrolls during training to directly penalize compound error. Then consider Phase 5 (Dreamer-style actor-critic on imagined rollouts).

---

## Phase 4c ‚Äî Multi-Step Rollout Training

**Date:** 2026-02-25 | **Compute:** Modal A10G, ~15 min

Hypothesis: the Phase 4b oscillation was caused by compounding 1-step prediction error over 10-step planner horizon. Fix: retrain dynamics model with **multi-step rollout loss** (H=5), backpropagating through predicted states:

```
z1_pred = f(z0, a0)           ‚Üí loss vs z1
z2_pred = f(z1_pred, a1)      ‚Üí loss vs z2  (uses PREDICTED z!)
z3_pred = f(z2_pred, a2)      ‚Üí loss vs z3
```

### Results

| Metric | Phase 4b (1-step) | Phase 4c (H=5) |
|---|---|---|
| Initial Latent Dist | 16.18 | 15.42 |
| Final Latent Dist | 9.50 | 10.57 |
| **Min Latent Dist** | **7.80** | 9.55 |
| **Improvement** | **41.3%** | 31.4% |
| Total Env Reward | 6.00 | 0.00 |

### Key findings

**‚ùå Multi-step training regressed!** The 1-step model (Phase 4b) outperformed the multi-step model (Phase 4c) on every metric. The multi-step model's min latent distance (9.55) never reached Phase 4b's (7.80).

**üí° Why?** Likely causes:
1. **Training instability** ‚Äî multi-step rollout loss creates long gradient chains that are harder to optimize. The model may learn conservative predictions that don't compound errors but also lack precision.
2. **Architecture bottleneck** ‚Äî the MLP dynamics model may not have enough capacity to benefit from multi-step supervision. A Transformer or larger model may be needed.
3. **Data diversity** ‚Äî 500 episodes of epsilon-greedy P-controller may not cover enough of the state space for multi-step learning.

**üí° Conclusion:** The dynamics model architecture (simple MLP) is likely the real bottleneck, not the training objective. Phase 5 (Dreamer-style actor-critic) should use a more expressive world model, or skip model-based planning entirely and use the learned latents for model-free RL.

---

## Blockers / Limitations

| Issue | Status | Impact |
|---|---|---|
| P-controller expert suboptimal | Known ‚Äî by design | Caps BC at 20%, P-controller never achieves high reward |
| dm_control reacher reward very sparse | Known | Even expert gets 0 reward; latent distance is better metric |
| Dynamics model compound error | Tested in Phase 4c | Multi-step training didn't help; MLP architecture is bottleneck |
| Large files not in git | `.gitignore` updated | `.npz` excluded; results/ now tracked |

---

## Cost Summary

| Step | Compute | Cost |
|---|---|---|
| Phase 1a: 500 demos + encoding | Modal A10G, ~35 min | ~$0.65 |
| Phase 1b: BC training | Local CPU, ~8 min | $0 |
| Phase 1c: Eval (2 conditions) | Modal A10G, ~60 min | ~$1.10 |
| Phase 2: Exploration data | Modal A10G, ~40 min | ~$0.75 |
| Phase 3: Dynamics training | Local CPU, ~10 min | $0 |
| Phase 4: Random shooting MPC | Local CPU, ~30 min | $0 |
| Phase 4b: CEM planner | Local CPU, ~60 min | $0 |
| Phase 4c: Multi-step dynamics | Modal A10G, ~15 min | ~$0.30 |
| **Total** | | **~$2.80** |
