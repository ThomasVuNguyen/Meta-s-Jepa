"""
Phase 4b â€” CEM-based Model Predictive Control (MPC) Planner
============================================================
Upgrades from Phase 4:
  1. Goal state generated by a P-controller expert (closest-to-target frame)
  2. Cross-Entropy Method (CEM) replaces random shooting for action optimization
  3. Tracks latent distance to goal as primary metric (env reward is very sparse)

At each step `t`:
1. Encode current image to `z_t`
2. CEM: iteratively refine N action trajectory candidates over K iterations
3. Use the trained dynamics predictor `f(z, a)` to unroll trajectories in latent space
4. Select the best action trajectory (lowest cost to z_goal)
5. Execute the first action of the winning trajectory
"""
import torch
import numpy as np
from dm_control import suite
from transformers import AutoModel
from PIL import Image
import torchvision.transforms as T
import imageio
from pathlib import Path
import matplotlib.pyplot as plt

# Import dynamics predictor from the training script
from train_dynamics import DynamicsPredictor

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

vjepa_transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def get_observation_image(time_step):
    pixels = time_step.observation['pixels']
    return Image.fromarray(pixels)

@torch.no_grad()
def encode_image(model, img: Image.Image):
    x = vjepa_transform(img)  # [3, 224, 224]
    window = [x] * 8
    clips_t = torch.stack(window).unsqueeze(0).to(DEVICE, dtype=torch.float16)
    outputs = model(pixel_values_videos=clips_t, return_dict=True)
    z = outputs.last_hidden_state.mean(dim=1)  # [1, 1024]
    return z


class CEMPlanner:
    """
    Cross-Entropy Method (CEM) planner.
    Iteratively refines action distribution:
      1. Sample N trajectories from Gaussian
      2. Evaluate via dynamics model
      3. Keep top-K elites
      4. Refit Gaussian to elites
      5. Repeat
    """
    def __init__(self, dynamics_model, num_samples=500, horizon=10,
                 action_dim=2, num_iterations=5, elite_frac=0.1):
        self.dynamics_model = dynamics_model
        self.num_samples = num_samples
        self.horizon = horizon
        self.action_dim = action_dim
        self.num_iterations = num_iterations
        self.num_elites = max(1, int(num_samples * elite_frac))
        self.action_low = -1.0
        self.action_high = 1.0
        self._prev_mean = None
    
    @torch.no_grad()
    def plan(self, z_t, z_goal):
        # Initialize distribution (warm-start from previous plan)
        if self._prev_mean is not None:
            mean = torch.zeros(self.horizon, self.action_dim, device=DEVICE)
            mean[:-1] = self._prev_mean[1:]
        else:
            mean = torch.zeros(self.horizon, self.action_dim, device=DEVICE)
        std = torch.ones(self.horizon, self.action_dim, device=DEVICE) * 0.5
        
        best_actions = None
        best_cost = float('inf')
        
        for iteration in range(self.num_iterations):
            noise = torch.randn(self.num_samples, self.horizon, self.action_dim, device=DEVICE)
            actions = (mean.unsqueeze(0) + std.unsqueeze(0) * noise).clamp(self.action_low, self.action_high)
            
            costs = self._evaluate(actions, z_t, z_goal)
            elite_idx = torch.topk(costs, self.num_elites, largest=False).indices
            elite_actions = actions[elite_idx]
            
            if costs[elite_idx[0]].item() < best_cost:
                best_cost = costs[elite_idx[0]].item()
                best_actions = actions[elite_idx[0]]
            
            mean = elite_actions.mean(dim=0)
            std = elite_actions.std(dim=0).clamp(min=0.05)
        
        self._prev_mean = mean.clone()
        return best_actions[0].cpu().numpy()
    
    def _evaluate(self, actions, z_t, z_goal):
        z_curr = z_t.expand(self.num_samples, -1)
        z_goal_exp = z_goal.expand(self.num_samples, -1)
        total_cost = torch.zeros(self.num_samples, device=DEVICE)
        
        for t in range(self.horizon):
            z_curr = self.dynamics_model(z_curr, actions[:, t, :])
            weight = (t + 1) / self.horizon
            total_cost += weight * torch.norm(z_curr - z_goal_exp, dim=-1)
        
        return total_cost


def generate_goal_with_expert(seed=42):
    """
    Use P-controller on a raw (unwrapped) env to find the closest-to-target state.
    Uses `to_target` distance (not reward) since dm_control's reward is very sparse.
    """
    print("Generating goal state using P-controller expert...")
    raw_env = suite.load(domain_name="reacher", task_name="easy", task_kwargs={'random': seed})
    ts = raw_env.reset()
    obs = ts.observation
    
    best_dist = float('inf')
    best_frame = None
    
    for step in range(300):
        action = np.clip(obs["to_target"] * 5.0, -1, 1).astype(np.float32)
        ts = raw_env.step(action)
        obs = ts.observation
        dist = np.linalg.norm(obs["to_target"])
        
        if dist < best_dist:
            best_dist = dist
            best_frame = raw_env.physics.render(height=224, width=224, camera_id=0).copy()
    
    print(f"  Best to_target distance: {best_dist:.4f}")
    goal_img = Image.fromarray(best_frame)
    return goal_img, best_dist


def main():
    Path("train_robots/results").mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("Phase 4b: CEM-based MPC Planner")
    print("=" * 60)
    
    print("\nLoading V-JEPA 2 model...")
    vjepa_model = AutoModel.from_pretrained(
        "facebook/vjepa2-vitl-fpc64-256", trust_remote_code=True
    ).to(DEVICE)
    vjepa_model.eval()

    print("Loading Dynamics Predictor...")
    dynamics_model = DynamicsPredictor(latent_dim=1024, action_dim=2, hidden_dim=512).to(DEVICE)
    dynamics_model.load_state_dict(
        torch.load("train_robots/models/dynamics_predictor.pt", weights_only=True)
    )
    dynamics_model.eval()

    # --- Fix 1: Expert-generated goal (on raw env) ---
    goal_img, goal_dist = generate_goal_with_expert(seed=42)
    goal_img.save("train_robots/results/mpc_goal_target.png")
    z_goal = encode_image(vjepa_model, goal_img)
    print(f"Goal encoded. Latent norm: {z_goal.norm().item():.2f}")

    # --- Init pixel-wrapped env for MPC ---
    print("Initializing pixel-wrapped environment...")
    env = suite.load(domain_name="reacher", task_name="easy", task_kwargs={'random': 42})
    from dm_control.suite.wrappers import pixels
    env = pixels.Wrapper(env, pixels_only=False, render_kwargs={
        'height': 224, 'width': 224, 'camera_id': 0
    })

    # --- Fix 2: CEM planner ---
    planner = CEMPlanner(
        dynamics_model,
        num_samples=500,
        horizon=10,
        num_iterations=5,
        elite_frac=0.1,
    )

    print("\n" + "=" * 60)
    print("Starting CEM-MPC Evaluation (200 steps)...")
    print("=" * 60)
    
    time_step = env.reset()
    
    # Encode initial state to get baseline latent distance
    init_img = get_observation_image(time_step)
    z_init = encode_image(vjepa_model, init_img)
    init_latent_dist = torch.norm(z_init - z_goal).item()
    print(f"Initial latent distance to goal: {init_latent_dist:.2f}")
    
    frames = []
    latent_distances = []
    rewards_log = []
    total_reward = 0.0
    
    max_steps = 200
    for step in range(max_steps):
        img_t = get_observation_image(time_step)
        frames.append(np.array(img_t))
        z_t = encode_image(vjepa_model, img_t)
        
        # Track latent distance to goal
        lat_dist = torch.norm(z_t - z_goal).item()
        latent_distances.append(lat_dist)
        
        # Plan with CEM
        action = planner.plan(z_t, z_goal)
        
        # Execute
        time_step = env.step(action)
        reward = time_step.reward or 0.0
        total_reward += reward
        rewards_log.append(reward)
        
        if (step + 1) % 10 == 0:
            print(f"Step {step+1:3d}/{max_steps} | "
                  f"Action: [{action[0]:5.2f}, {action[1]:5.2f}] | "
                  f"Latent Dist: {lat_dist:.2f} | "
                  f"Env Reward: {reward:.3f}")
    
    # Final encoding
    final_img = get_observation_image(time_step)
    z_final = encode_image(vjepa_model, final_img)
    final_latent_dist = torch.norm(z_final - z_goal).item()
    
    print(f"\n{'=' * 60}")
    print(f"Evaluation Complete.")
    print(f"  Initial Latent Dist:  {init_latent_dist:.2f}")
    print(f"  Final Latent Dist:    {final_latent_dist:.2f}")
    print(f"  Min Latent Dist:      {min(latent_distances):.2f}")
    print(f"  Improvement:          {(1 - final_latent_dist / init_latent_dist) * 100:.1f}%")
    print(f"  Total Env Reward:     {total_reward:.2f}")
    print(f"{'=' * 60}")
    
    # Save video
    out_video = "train_robots/results/mpc_cem_rollout.mp4"
    imageio.mimwrite(out_video, frames, fps=30)
    print(f"\nSaved CEM-MPC rollout to {out_video}")
    
    # Save plots
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    axes[0].plot(latent_distances, color='#4361ee', linewidth=1.5)
    axes[0].axhline(y=init_latent_dist, color='red', linestyle='--', alpha=0.5, label='Initial dist')
    axes[0].set_xlabel("Step")
    axes[0].set_ylabel("Latent Distance to Goal")
    axes[0].set_title("Latent Distance Over Time (lower = closer to goal)")
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    axes[1].plot(np.cumsum(rewards_log), color='#f72585', linewidth=1.5)
    axes[1].set_xlabel("Step")
    axes[1].set_ylabel("Cumulative Env Reward")
    axes[1].set_title("Cumulative Environment Reward")
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig("train_robots/results/mpc_cem_metrics.png", dpi=150)
    print("Saved metrics plot to train_robots/results/mpc_cem_metrics.png")


if __name__ == "__main__":
    main()
